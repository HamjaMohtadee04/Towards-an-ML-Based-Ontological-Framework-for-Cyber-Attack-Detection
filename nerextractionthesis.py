# -*- coding: utf-8 -*-
"""NERextractionthesis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eq43oVinb2Zh64rPzd6RLcpH9O6GfQZ4
"""



from google.colab import drive
drive.mount('/content/drive')

import spacy
import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv", sep='\t')

!file "/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv"

!iconv -f <original_encoding> -t UTF-8 "/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv" > "/content/drive/MyDrive/archive (1)/NUSW-NB15_features_utf8.csv"

df = pd.read_csv("/content/drive/MyDrive/archive (1)/NUSW-NB15_features_utf8.csv", sep='\t')

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv", encoding='latin1')

df

!pip install spacy
!python -m spacy download en_core_web_sm

import pandas as pd
import spacy

# Load the SpaCy model
nlp = spacy.load("en_core_web_sm")

# Read the CSV file
df = pd.read_csv("/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv", sep='\t', encoding='latin1')

# Combine all the text data into a single string for NER
text_data = ' '.join(df['Description'].astype(str))

# Process the text data using SpaCy
doc = nlp(text_data)

# Extract named entities
entities = [(ent.text, ent.label_) for ent in doc.ents]

# Print the extracted entities
for entity in entities:
    print(entity)

import pandas as pd
import spacy

# Load the SpaCy model
nlp = spacy.load("en_core_web_sm")

# Read the CSV file
df = pd.read_csv("/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv", sep='\t', encoding='latin1')

# Print the column names to verify
print(df.columns)

# Assuming the column containing text data is named 'Description', if not, replace with the correct column name
# Combine all the text data into a single string for NER
text_data = 'Description' '.join(df[''].astype(str))

# Process the text data using SpaCy
doc = nlp(text_data)

# Extract named entities
entities = [(ent.text, ent.label_) for ent in doc.ents]

# Print the extracted entities
for entity in entities:
    print(entity)

print(df.columns)

import pandas as pd
import spacy

# Load the SpaCy model
nlp = spacy.load("en_core_web_sm")

# Read the CSV file
df = pd.read_csv("/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv", sep='\t', encoding='latin1')

# Print the column names to verify
print(df.columns)

# Assuming the column containing text data is named 'Description', if not, replace with the correct column name
# Combine all the text data into a single string for NER
text_data = ' '.join(df['Description'].astype(str))

# Process the text data using SpaCy
doc = nlp(text_data)

# Extract named entities
entities = [(ent.text, ent.label_) for ent in doc.ents]

# Print the extracted entities
for entity in entities:
    print(entity)

import pandas as pd
import spacy

# Load the SpaCy model
nlp = spacy.load("en_core_web_sm")

# Read the CSV file with correct settings
df = pd.read_csv("/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv", sep=',', encoding='latin1', header=0)

# Print the first few rows to verify the correct parsing of columns
print(df.head())

# Verify column names
print(df.columns)

# Assuming the column containing text data is named 'Description', if not, replace with the correct column name
# Combine all the text data into a single string for NER
text_data = ' '.join(df['Description'].astype(str))

# Process the text data using SpaCy
doc = nlp(text_data)

# Extract named entities
entities = [(ent.text, ent.label_) for ent in doc.ents]

# Print the extracted entities
for entity in entities:
    print(entity)





import spacy
from spacy.training import Example
from spacy.util import minibatch, compounding
import random

# Sample training data
TRAIN_DATA = [
    ("Source IP address", {"entities": [(0, 17, "ORG")]}),
    ("Reconnaissance", {"entities": [(0, 13, "ATTACK")]}),
    # Add more annotated examples here
]

# Load a blank SpaCy model
nlp = spacy.blank("en")

# Create a new NER component and add it to the pipeline
if "ner" not in nlp.pipe_names:
    ner = nlp.add_pipe("ner")
else:
    ner = nlp.get_pipe("ner")

# Add the new labels to the NER component
for _, annotations in TRAIN_DATA:
    for ent in annotations.get("entities"):
        ner.add_label(ent[2])

# Disable other pipes in the pipeline during training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != "ner"]
with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()
    for itn in range(100):  # Number of training iterations
        random.shuffle(TRAIN_DATA)
        losses = {}
        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            for text, annotations in batch:
                example = Example.from_dict(nlp.make_doc(text), annotations)
                nlp.update([example], losses=losses, drop=0.5)
        print(f"Iteration {itn} - Losses: {losses}")

# Save the trained model
nlp.to_disk("custom_ner_model")

# Load the trained model and test it
custom_nlp = spacy.load("custom_ner_model")
doc = custom_nlp("Reconnaissance and Source IP address")
for ent in doc.ents:
    print(ent.text, ent.label_)

import pandas as pd
import spacy

# Load the SpaCy model
nlp = spacy.load("en_core_web_sm")

# Read the CSV file with correct settings
df = pd.read_csv("/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv", sep=',', encoding='latin1', header=0)

# Print the first few rows to verify the correct parsing of columns
print(df.head())

# Verify column names
print(df.columns)

# Assuming the column containing text data is named 'Description', if not, replace with the correct column name
# Combine all the text data into a single string for NER
text_data = ' '.join(df['Description'].astype(str))

# Process the text data using SpaCy
doc = nlp(text_data)

# Extract named entities
entities = [(ent.text, ent.label_) for ent in doc.ents]

# Print the extracted entities
for entity in entities:
    print(entity)

import spacy
from spacy.training import Example
from spacy.training.iob_utils import offsets_to_biluo_tags
from spacy.util import minibatch, compounding
import random

# Function to check and adjust entity offsets
def validate_and_adjust_entities(nlp, text, entities):
    doc = nlp.make_doc(text)
    tags = offsets_to_biluo_tags(doc, entities)
    for i, tag in enumerate(tags):
        if tag == '-':
            print(f"Warning: Misaligned entity in '{text}' at index {i}")
            return False
    return True

# Sample training data
TRAIN_DATA = [
    ("Source IP address", {"entities": [(0, 17, "ORG")]}),
    ("Reconnaissance", {"entities": [(0, 13, "ATTACK")]}),
    # Add more annotated examples here
]

# Load a blank SpaCy model
nlp = spacy.blank("en")

# Create a new NER component and add it to the pipeline
if "ner" not in nlp.pipe_names:
    ner = nlp.add_pipe("ner")
else:
    ner = nlp.get_pipe("ner")

# Add the new labels to the NER component
for _, annotations in TRAIN_DATA:
    for ent in annotations.get("entities"):
        ner.add_label(ent[2])

# Validate training data
validated_data = []
for text, annotations in TRAIN_DATA:
    if validate_and_adjust_entities(nlp, text, annotations["entities"]):
        validated_data.append((text, annotations))

# Disable other pipes in the pipeline during training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != "ner"]
with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()
    for itn in range(100):  # Number of training iterations
        random.shuffle(validated_data)
        losses = {}
        batches = minibatch(validated_data, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            for text, annotations in batch:
                example = Example.from_dict(nlp.make_doc(text), annotations)
                nlp.update([example], losses=losses, drop=0.5)
        print(f"Iteration {itn} - Losses: {losses}")

# Save the trained model
nlp.to_disk("custom_ner_model")

# Load the trained model and test it
custom_nlp = spacy.load("custom_ner_model")
doc = custom_nlp("Reconnaissance and Source IP address")
for ent in doc.ents:
    print(ent.text, ent.label_)

import spacy
from spacy.training import Example
from spacy.training.iob_utils import offsets_to_biluo_tags
from spacy.util import minibatch, compounding
import random

# Function to check and adjust entity offsets
def validate_and_adjust_entities(nlp, text, entities):
    doc = nlp.make_doc(text)
    tags = offsets_to_biluo_tags(doc, entities)
    new_entities = []
    for i, tag in enumerate(tags):
        if tag == '-':
            print(f"Warning: Misaligned entity in '{text}' at index {i}")
            # Adjust entity to align with token boundaries
            entity = entities[0]
            start, end, label = entity
            start = doc[start:end].start_char
            end = doc[start:end].end_char
            new_entities.append((start, end, label))
        else:
            new_entities.append(entities[i])
    return new_entities

# Sample training data
TRAIN_DATA = [
    ("Source IP address", {"entities": [(0, 17, "ORG")]}),
    ("Reconnaissance", {"entities": [(0, 13, "ATTACK")]}),
    # Add more annotated examples here
]

# Load a blank SpaCy model
nlp = spacy.blank("en")

# Create a new NER component and add it to the pipeline
if "ner" not in nlp.pipe_names:
    ner = nlp.add_pipe("ner")
else:
    ner = nlp.get_pipe("ner")

# Add the new labels to the NER component
for _, annotations in TRAIN_DATA:
    for ent in annotations.get("entities"):
        ner.add_label(ent[2])

# Validate training data
validated_data = []
for text, annotations in TRAIN_DATA:
    adjusted_entities = validate_and_adjust_entities(nlp, text, annotations["entities"])
    validated_data.append((text, {"entities": adjusted_entities}))

# Disable other pipes in the pipeline during training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != "ner"]
with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()
    for itn in range(100):  # Number of training iterations
        random.shuffle(validated_data)
        losses = {}
        batches = minibatch(validated_data, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            for text, annotations in batch:
                example = Example.from_dict(nlp.make_doc(text), annotations)
                nlp.update([example], losses=losses, drop=0.5)
        print(f"Iteration {itn} - Losses: {losses}")

# Save the trained model
nlp.to_disk("custom_ner_model")

# Load the trained model and test it
custom_nlp = spacy.load("custom_ner_model")
doc = custom_nlp("Reconnaissance and Source IP address")
for ent in doc.ents:
    print(ent.text, ent.label_)

import spacy
from spacy.training import Example
from spacy.training.iob_utils import offsets_to_biluo_tags
from spacy.util import minibatch, compounding
import random

# Function to check and adjust entity offsets
def validate_and_adjust_entities(nlp, text, entities):
    doc = nlp.make_doc(text)
    tags = offsets_to_biluo_tags(doc, entities)
    valid_entities = []
    current_entity = None

    for i, tag in enumerate(tags):
        if tag.startswith("B"):
            start = doc[i].idx
            end = doc[i].idx + len(doc[i])
            current_entity = [start, end, tag[2:]]
        elif tag.startswith("I") and current_entity:
            current_entity[1] = doc[i].idx + len(doc[i])
        elif tag.startswith("L") and current_entity:
            current_entity[1] = doc[i].idx + len(doc[i])
            valid_entities.append(tuple(current_entity))
            current_entity = None
        elif tag == "U":
            start = doc[i].idx
            end = doc[i].idx + len(doc[i])
            valid_entities.append((start, end, tag[2:]))
        elif tag == "O":
            current_entity = None

    if current_entity:  # handle the case where the entity is not properly closed
        valid_entities.append(tuple(current_entity))

    if len(valid_entities) != len(entities):
        print(f"Warning: Adjusted entities do not match original in '{text}'")

    return valid_entities

# Sample training data
TRAIN_DATA = [
    ("Source IP address", {"entities": [(0, 17, "ORG")]}),
    ("Reconnaissance", {"entities": [(0, 13, "ATTACK")]}),
    # Add more annotated examples here
]

# Load a blank SpaCy model
nlp = spacy.blank("en")

# Create a new NER component and add it to the pipeline
if "ner" not in nlp.pipe_names:
    ner = nlp.add_pipe("ner")
else:
    ner = nlp.get_pipe("ner")

# Add the new labels to the NER component
for _, annotations in TRAIN_DATA:
    for ent in annotations.get("entities"):
        ner.add_label(ent[2])

# Validate training data
validated_data = []
for text, annotations in TRAIN_DATA:
    adjusted_entities = validate_and_adjust_entities(nlp, text, annotations["entities"])
    validated_data.append((text, {"entities": adjusted_entities}))

# Disable other pipes in the pipeline during training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != "ner"]
with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()
    for itn in range(100):  # Number of training iterations
        random.shuffle(validated_data)
        losses = {}
        batches = minibatch(validated_data, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            for text, annotations in batch:
                example = Example.from_dict(nlp.make_doc(text), annotations)
                nlp.update([example], losses=losses, drop=0.5)
        print(f"Iteration {itn} - Losses: {losses}")

# Save the trained model
nlp.to_disk("custom_ner_model")

# Load the trained model and test it
custom_nlp = spacy.load("custom_ner_model")
doc = custom_nlp("Reconnaissance and Source IP address")
for ent in doc.ents:
    print(ent.text, ent.label_)

import spacy
from spacy.training import Example
from spacy.util import minibatch, compounding
import random

# Sample training data
TRAIN_DATA = [
    ("Fuzzers, Analysis, Backdoors, DoS Exploits, Generic, Reconnaissance, Shellcode and Worms", {"entities": [(0, 7, "ATTACK"), (9, 17, "ATTACK"), (19, 28, "ATTACK"), (30, 42, "ATTACK"), (44, 51, "ATTACK"), (53, 66, "ATTACK"), (68, 77, "ATTACK"), (82, 87, "ATTACK")]}),
    # Add more annotated examples here
]

# Load a blank SpaCy model
nlp = spacy.blank("en")

# Create a new NER component and add it to the pipeline
if "ner" not in nlp.pipe_names:
    ner = nlp.add_pipe("ner")
else:
    ner = nlp.get_pipe("ner")

# Add the new labels to the NER component
for _, annotations in TRAIN_DATA:
    for ent in annotations.get("entities"):
        ner.add_label(ent[2])

# Disable other pipes in the pipeline during training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != "ner"]
with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()
    for itn in range(100):  # Number of training iterations
        random.shuffle(TRAIN_DATA)
        losses = {}
        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            for text, annotations in batch:
                example = Example.from_dict(nlp.make_doc(text), annotations)
                nlp.update([example], losses=losses, drop=0.5)
        print(f"Iteration {itn} - Losses: {losses}")

# Save the trained model
nlp.to_disk("custom_ner_model")

# Load the trained model and test it
custom_nlp = spacy.load("custom_ner_model")

# Test data
test_text = "Fuzzers, Analysis, Backdoors, DoS Exploits, Generic, Reconnaissance, Shellcode and Worms"
doc = custom_nlp(test_text)
for ent in doc.ents:
    print(ent.text, ent.label_)

import spacy
from spacy.training import Example
from spacy.util import minibatch, compounding
import random

# Sample training data
TRAIN_DATA = [
    ("Fuzzers, Analysis, Backdoors, DoS Exploits, Generic, Reconnaissance, Shellcode and Worms",
     {"entities": [
         (0, 7, "ATTACK"),
         (9, 17, "ATTACK"),
         (19, 28, "ATTACK"),
         (30, 42, "ATTACK"),
         (44, 51, "ATTACK"),
         (53, 66, "ATTACK"),
         (68, 77, "ATTACK"),
         (82, 87, "ATTACK")
     ]}
    ),
    # Add more annotated examples here
]

# Load a blank SpaCy model
nlp = spacy.blank("en")

# Create a new NER component and add it to the pipeline
if "ner" not in nlp.pipe_names:
    ner = nlp.add_pipe("ner")
else:
    ner = nlp.get_pipe("ner")

# Add the new labels to the NER component
for _, annotations in TRAIN_DATA:
    for ent in annotations.get("entities"):
        ner.add_label(ent[2])

# Disable other pipes in the pipeline during training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != "ner"]
with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()
    for itn in range(100):  # Number of training iterations
        random.shuffle(TRAIN_DATA)
        losses = {}
        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            for text, annotations in batch:
                example = Example.from_dict(nlp.make_doc(text), annotations)
                nlp.update([example], losses=losses, drop=0.5)
        print(f"Iteration {itn} - Losses: {losses}")

# Save the trained model
nlp.to_disk("custom_ner_model")

# Load the trained model and test it
custom_nlp = spacy.load("custom_ner_model")

# Test data
test_text = "Fuzzers, Analysis, Backdoors, DoS Exploits, Generic, Reconnaissance, Shellcode and Worms"
doc = custom_nlp(test_text)

# Print the names of the attack categories
attack_categories = [ent.text for ent in doc.ents if ent.label_ == "ATTACK"]
print("Attack Categories:", ", ".join(attack_categories))

import pandas as pd
import spacy

# Load the SpaCy model
nlp = spacy.load("en_core_web_sm")

# Read the CSV file with correct settings
df = pd.read_csv("/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv", sep=',', encoding='latin1', header=0)

# Print the first few rows to verify the correct parsing of columns
print(df.head())

# Verify column names
print(df.columns)

# Assuming the column containing text data is named 'Description', if not, replace with the correct column name
# Combine all the text data into a single string for NER
text_data = ' '.join(df['Description'].astype(str))

# Process the text data using SpaCy
doc = nlp(text_data)

# Extract named entities
entities = [(ent.text, ent.label_) for ent in doc.ents]

# Print the extracted entities
for entity in entities:
    print(entity)







import spacy
from spacy.training import Example
from spacy.util import minibatch, compounding
import random

# Sample training data
TRAIN_DATA = [
    ("Fuzzers, Analysis, Backdoors, DoS Exploits, Generic, Reconnaissance, Shellcode and Worms",
     {"entities": [
         (0, 7, "ATTACK"),
         (9, 17, "ATTACK"),
         (19, 28, "ATTACK"),
         (30, 42, "ATTACK"),
         (44, 51, "ATTACK"),
         (53, 66, "ATTACK"),
         (68, 77, "ATTACK"),
         (82, 87, "ATTACK")
     ]}
    ),
    # Add more annotated examples here if needed
]

# Load the pre-trained SpaCy model
nlp = spacy.load("en_core_web_sm")

# Get the NER component
ner = nlp.get_pipe("ner")

# Add the new "ATTACK" label to the NER component
for _, annotations in TRAIN_DATA:
    for ent in annotations.get("entities"):
        ner.add_label(ent[2])

# Disable other pipes during training to only train the NER component
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != "ner"]
with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.resume_training()
    for itn in range(100):  # Number of training iterations
        random.shuffle(TRAIN_DATA)
        losses = {}
        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            for text, annotations in batch:
                example = Example.from_dict(nlp.make_doc(text), annotations)
                nlp.update([example], losses=losses, drop=0.5)
        print(f"Iteration {itn} - Losses: {losses}")

# Save the updated model
nlp.to_disk("custom_ner_model")

# Load the updated model and test it
custom_nlp = spacy.load("custom_ner_model")

# Test data
test_text = "Fuzzers, Analysis, Backdoors, DoS Exploits, Generic, Reconnaissance, Shellcode and Worms"
doc = custom_nlp(test_text)

# Print the entities with their labels
for ent in doc.ents:
    print(ent.text, ent.label_)

import spacy
from spacy.training import Example
from spacy.util import minibatch, compounding
import random

# Sample training data
TRAIN_DATA = [
    ("Fuzzers, Analysis, Backdoors, DoS Exploits, Generic, Reconnaissance, Shellcode and Worms",
     {"entities": [
         (0, 7, "ATTACK"),
         (9, 17, "ATTACK"),
         (19, 28, "ATTACK"),
         (30, 42, "ATTACK"),
         (44, 51, "ATTACK"),
         (53, 66, "ATTACK"),
         (68, 77, "ATTACK"),
         (82, 87, "ATTACK")
     ]}
    ),
    # Add more annotated examples here if needed
]

# Load the pre-trained SpaCy model
nlp = spacy.load("en_core_web_sm")

# Get the NER component
ner = nlp.get_pipe("ner")

# Add the new "ATTACK" label to the NER component
for _, annotations in TRAIN_DATA:
    for ent in annotations.get("entities"):
        ner.add_label(ent[2])

# Resume training the NER component
optimizer = nlp.resume_training()
for itn in range(100):  # Number of training iterations
    random.shuffle(TRAIN_DATA)
    losses = {}
    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
    for batch in batches:
        for text, annotations in batch:
            example = Example.from_dict(nlp.make_doc(text), annotations)
            nlp.update([example], losses=losses, drop=0.5)

# Save the updated model
nlp.to_disk("custom_ner_model")

# Load the updated model and test it
custom_nlp = spacy.load("custom_ner_model")

# Test data
test_text = "Fuzzers, Analysis, Backdoors, DoS Exploits, Generic, Reconnaissance, Shellcode and Worms"
doc = custom_nlp(test_text)

# Print the entities with their labels
for ent in doc.ents:
    print(ent.text, ent.label_)

import pandas as pd
import spacy

# Load the custom SpaCy model
custom_nlp = spacy.load("custom_ner_model")

# Read the CSV file with correct settings
df = pd.read_csv("/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv", sep=',', encoding='latin1', header=0)

# Print the first few rows to verify the correct parsing of columns
print(df.head())

# Verify column names
print(df.columns)

# Assuming the column containing text data is named 'Description', if not, replace with the correct column name
# Combine all the text data into a single string for NER
text_data = ' '.join(df['Description'].astype(str))

# Process the text data using the custom SpaCy model
doc = custom_nlp(text_data)

# Extract named entities
entities = [(ent.text, ent.label_) for ent in doc.ents]

# Print the extracted entities
for entity in entities:
    print(entity)

import spacy
from spacy.training import Example
from spacy.util import minibatch, compounding
import random
import pandas as pd

# Define the training data
TRAIN_DATA = [
    ("Fuzzers, Analysis, Backdoors, DoS Exploits, Generic, Reconnaissance, Shellcode and Worms",
     {"entities": [
         (0, 7, "ATTACK"),
         (9, 17, "ATTACK"),
         (19, 28, "ATTACK"),
         (30, 42, "ATTACK"),
         (44, 51, "ATTACK"),
         (53, 66, "ATTACK"),
         (68, 77, "ATTACK"),
         (82, 87, "ATTACK")
     ]}
    ),
    # Add more annotated examples if needed
]

# Load the pre-trained SpaCy model
nlp = spacy.load("en_core_web_sm")

# Get the NER component
ner = nlp.get_pipe("ner")

# Add the new "ATTACK" label to the NER component
for _, annotations in TRAIN_DATA:
    for ent in annotations.get("entities"):
        ner.add_label(ent[2])

# Disable other components (if needed) and resume training
with nlp.select_pipes(enable=["ner"]):
    optimizer = nlp.resume_training()
    for itn in range(100):  # Number of training iterations
        random.shuffle(TRAIN_DATA)
        losses = {}
        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            for text, annotations in batch:
                example = Example.from_dict(nlp.make_doc(text), annotations)
                nlp.update([example], losses=losses, drop=0.5)
        print(f"Iteration {itn} - Losses: {losses}")

# Save the updated model
nlp.to_disk("custom_ner_model")

# Load the custom SpaCy model
custom_nlp = spacy.load("custom_ner_model")

# Read the CSV file
df = pd.read_csv("/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv", sep=',', encoding='latin1', header=0)

# Print the first few rows to verify the correct parsing of columns
print(df.head())

# Verify column names
print(df.columns)

# Assuming the column containing text data is named 'Description', if not, replace with the correct column name
# Combine all the text data into a single string for NER
text_data = ' '.join(df['Description'].astype(str))

# Process the text data using the custom SpaCy model
doc = custom_nlp(text_data)

# Extract named entities
entities = [(ent.text, ent.label_) for ent in doc.ents]

# Print the extracted entities
for entity in entities:
    print(entity)

import spacy
from spacy.training import Example
from spacy.util import minibatch, compounding
import random
import pandas as pd

# Define the training data with more diverse examples
TRAIN_DATA = [
    ("Fuzzers, Analysis, Backdoors, DoS Exploits, Generic, Reconnaissance, Shellcode and Worms are types of attacks.",
     {"entities": [
         (0, 7, "ATTACK"),
         (9, 17, "ATTACK"),
         (19, 28, "ATTACK"),
         (30, 42, "ATTACK"),
         (44, 51, "ATTACK"),
         (53, 66, "ATTACK"),
         (68, 77, "ATTACK"),
         (82, 87, "ATTACK")
     ]}
    ),
    ("The IP address 192.168.0.1 is often used in local networks.",
     {"entities": [
         (12, 24, "GPE"),
         (25, 36, "CARDINAL")
     ]}
    ),
    ("The protocol used is TCP, which is reliable.",
     {"entities": [
         (21, 24, "PROTOCOL")
     ]}
    ),
    # Add more annotated examples for other entity types
]

# Load the pre-trained SpaCy model
nlp = spacy.load("en_core_web_sm")

# Get the NER component
ner = nlp.get_pipe("ner")

# Add the new "ATTACK" label to the NER component
for _, annotations in TRAIN_DATA:
    for ent in annotations.get("entities"):
        ner.add_label(ent[2])

# Disable other components (if needed) and resume training
with nlp.select_pipes(enable=["ner"]):
    optimizer = nlp.resume_training()
    for itn in range(20):  # Reduced number of training iterations for brevity
        random.shuffle(TRAIN_DATA)
        losses = {}
        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            for text, annotations in batch:
                example = Example.from_dict(nlp.make_doc(text), annotations)
                nlp.update([example], losses=losses, drop=0.5)
        print(f"Iteration {itn} - Losses: {losses}")

# Save the updated model
nlp.to_disk("custom_ner_model")

# Load the custom SpaCy model
custom_nlp = spacy.load("custom_ner_model")

# Read the CSV file
df = pd.read_csv("/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv", sep=',', encoding='latin1', header=0)

# Print the first few rows to verify the correct parsing of columns
print(df.head())

# Verify column names
print(df.columns)

# Assuming the column containing text data is named 'Description', if not, replace with the correct column name
# Combine all the text data into a single string for NER
text_data = ' '.join(df['Description'].astype(str))

# Process the text data using the custom SpaCy model
doc = custom_nlp(text_data)

# Extract named entities
entities = [(ent.text, ent.label_) for ent in doc.ents]

# Print the extracted entities
for entity in entities:
    print(entity)

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv', sep=',', encoding='latin1', header=0)

# Print the first few rows to inspect the dataset
print(df.head())

# Print the column names to verify the 'Description' column
print(df.columns)

import spacy

# Load the SpaCy model
nlp = spacy.load("en_core_web_sm")

# Combine all the text data from the 'Description' column into a single string for NER
text_data = ' '.join(df['Description'].astype(str))

# Process the text data using SpaCy
doc = nlp(text_data)

# Extract named entities
entities = [(ent.text, ent.label_) for ent in doc.ents]

# Print the extracted entities
for entity in entities:
    print(entity)

import spacy
from spacy.training import Example
from spacy.util import minibatch, compounding
import random
import pandas as pd

# Define the training data with specific annotations
TRAIN_DATA = [
    ("Fuzzers, Analysis, Backdoors, DoS Exploits, Generic, Reconnaissance, Shellcode and Worms",
     {"entities": [
         (0, 7, "ATTACK"),
         (9, 17, "ATTACK"),
         (19, 28, "ATTACK"),
         (30, 42, "ATTACK"),
         (44, 51, "ATTACK"),
         (53, 66, "ATTACK"),
         (68, 77, "ATTACK"),
         (79, 85, "ATTACK")
     ]}
    ),
    ("http, ftp, smtp, ssh, dns, TCP",
     {"entities": [
         (0, 4, "PROTOCOL"),
         (6, 9, "PROTOCOL"),
         (11, 15, "PROTOCOL"),
         (17, 20, "PROTOCOL"),
         (22, 25, "PROTOCOL"),
         (27, 30, "PROTOCOL")
     ]}
    ),
    # Add more annotated examples if needed
]

# Load the pre-trained SpaCy model
nlp = spacy.load("en_core_web_sm")

# Get the NER component
ner = nlp.get_pipe("ner")

# Add the new "ATTACK" and "PROTOCOL" labels to the NER component
for _, annotations in TRAIN_DATA:
    for ent in annotations.get("entities"):
        ner.add_label(ent[2])

# Disable other components (if needed) and resume training
with nlp.select_pipes(enable=["ner"]):
    optimizer = nlp.resume_training()
    for itn in range(100):  # Number of training iterations
        random.shuffle(TRAIN_DATA)
        losses = {}
        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            for text, annotations in batch:
                example = Example.from_dict(nlp.make_doc(text), annotations)
                nlp.update([example], losses=losses, drop=0.5)
        print(f"Iteration {itn} - Losses: {losses}")

# Save the updated model
nlp.to_disk("custom_ner_model")

# Load the custom SpaCy model
custom_nlp = spacy.load("custom_ner_model")

# Read the CSV file
df = pd.read_csv("/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv", sep=',', encoding='latin1', header=0)

# Combine all the text data from the 'Description' column into a single string for NER
text_data = ' '.join(df['Description'].astype(str))

# Process the text data using the custom SpaCy model
doc = custom_nlp(text_data)

# Extract named entities
entities = [(ent.text, ent.label_) for ent in doc.ents]

# Print the extracted entities
for entity in entities:
    print(entity)

import spacy
from spacy.training import Example
from spacy.util import minibatch, compounding
import random
import pandas as pd

# Define the training data with specific annotations
TRAIN_DATA = [
    ("Fuzzers, Analysis, Backdoors, DoS Exploits, Generic, Reconnaissance, Shellcode and Worms",
     {"entities": [
         (0, 7, "ATTACK"),
         (9, 17, "ATTACK"),
         (19, 28, "ATTACK"),
         (30, 42, "ATTACK"),
         (44, 51, "ATTACK"),
         (53, 66, "ATTACK"),
         (68, 77, "ATTACK"),
         (79, 85, "ATTACK")
     ]}
    ),
    ("http, ftp, smtp, ssh, dns, TCP",
     {"entities": [
         (0, 4, "PROTOCOL"),
         (6, 9, "PROTOCOL"),
         (11, 15, "PROTOCOL"),
         (17, 20, "PROTOCOL"),
         (22, 25, "PROTOCOL"),
         (27, 30, "PROTOCOL")
     ]}
    ),
    # Add more annotated examples if needed
]

# Load the pre-trained SpaCy model
nlp = spacy.load("en_core_web_sm")

# Get the NER component
ner = nlp.get_pipe("ner")

# Add the new "ATTACK" and "PROTOCOL" labels to the NER component
for _, annotations in TRAIN_DATA:
    for ent in annotations.get("entities"):
        ner.add_label(ent[2])

# Disable other components and train only NER
with nlp.select_pipes(enable=["ner"]):
    optimizer = nlp.resume_training()
    for itn in range(100):  # Number of training iterations
        random.shuffle(TRAIN_DATA)
        losses = {}
        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            for text, annotations in batch:
                example = Example.from_dict(nlp.make_doc(text), annotations)
                nlp.update([example], losses=losses, drop=0.5)
        print(f"Iteration {itn} - Losses: {losses}")

# Save the updated model
nlp.to_disk("custom_ner_model")

# Load the custom SpaCy model
custom_nlp = spacy.load("custom_ner_model")

# Read the CSV file
df = pd.read_csv("/content/drive/MyDrive/archive (1)/NUSW-NB15_features.csv", sep=',', encoding='latin1', header=0)

# Combine all the text data from the 'Description' column into a single string for NER
text_data = ' '.join(df['Description'].astype(str))

# Process the text data using the custom SpaCy model
doc = custom_nlp(text_data)

# Extract named entities
entities = [(ent.text, ent.label_) for ent in doc.ents]

# Print the extracted entities
for entity in entities:
    print(entity)









